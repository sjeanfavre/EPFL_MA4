{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students' name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aim of this course is to review the evolution of image processing tools from hand-crafted methods to deep learning algorithms. The semester is split into four labs :\n",
    "\n",
    "* **Lab 1** : Introduction to Image Processing Using Hand-Crafted Features\n",
    "* **Lab 2** : Object detection\n",
    "* **Lab 3** : Object tracking\n",
    "* **Lab 4** : Introduction to Deep Learning for image classification\n",
    "\n",
    "Let's start with the first chapter of this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 : Introduction to Image Processing Using Hand-Crafted Features \n",
    "\n",
    "(100 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to Basic Image Processing Using OpenCV\n",
    "\n",
    "(20 points)\n",
    "\n",
    "In this section we'll start with the basic image processing in Python and show you how you can load an image, perform some basic processing on the image, changing the color spaces , using the threshold and color histogram.\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "1. Read an image, access and modify pixel values\n",
    "2. Set Region of Image (*ROI*)\n",
    "3. Change color space\n",
    "4. Understand the concepts of histogram and histogram equalization\n",
    "\n",
    "Almost all the operations in this section are mainly related to Numpy rather than OpenCV. A good knowledge of Numpy is required to write better optimized code with OpenCV.\n",
    "\n",
    "For more information check the [numpy](https://docs.scipy.org/doc/) and [opencv](https://www.docs.opencv.org/3.4.0/) documentations.\n",
    "\n",
    "### 1.1.1 Accessing and Modifying pixel values\n",
    "\n",
    "The code in the next section shows how one can load an image into the Jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading an image\n",
    "img = cv.imread(os.path.join('..','data', 'lena.png'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "print('There is a total of {} elements'.format(img.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements of numpy array can be accessed through indexing, similar to Matlab. To do so, use the operator `[.]` where `.` indicates which dimensions you're accessing. OpenCV loads image with dimensions organised as follow: `rows, cols, channels`. More information about indexing in numpy can be found in the [doc](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing RED value\n",
    "print('Red value at location {}x{} is {}'.format(10, 10, img[10, 10, 2]))\n",
    "# modifying RED value\n",
    "img[10,10,2] = 100\n",
    "print('The new Red value at location {}x{} is {}'.format(10, 10, img.item(10,10,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image can be displayed using the provided function `display_image()` in file `utils.py`. It takes care of the conversion between OpenCV formatting and Matplotlib package for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image can be displayed like thiss\n",
    "display_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Image ROI\n",
    "\n",
    "Sometimes, you will have to play with certain region of images. For eye detection in images, first perform face detection over the image until the face is found, then search within the face region for eyes. This approach improves accuracy (because eyes are always on faces :D ) and performance (because we search for a small area).\n",
    "\n",
    "ROI is again obtained using Numpy indexing. Here I am selecting Lena's face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = img[200:375, 200:375, :]\n",
    "display_image(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Colorspace Conversion\n",
    "\n",
    "In this tutorial, you will learn how to convert images from one colorspace to another, like BGR ↔ Gray, BGR ↔ YUV etc. In addition to that, we will create an application which extracts a colored object within an image. There are more than 150 color-space conversion methods available in OpenCV. But we will look into only two which are most widely used ones, BGR ↔ Gray and BGR ↔ YUV. \n",
    "\n",
    "In the next two cells, implements the conversion function going from BGR image to GRAY image and BGR to YUV image. There are built-in functions in OpenCV doing exactly that but for the moment we'll ask you to implement your **own** function in order to get the idea of what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_grayscale(frame):\n",
    "    \"\"\"\n",
    "    Convert a given rgb image into grayscale image\n",
    "    :param frame: Color image to convert\n",
    "    :return: Graysale image as numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # CODE HERE\n",
    "    \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_yuv(frame):\n",
    "    \"\"\"\n",
    "    Convert a given rgb image into hsv image\n",
    "    :param frame: Color image to convert\n",
    "    :return: YUV image as numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # CODE HERE\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines shows the conversion based on your implementation and the built-in function from OpenCV. If everything went well, the two should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'lena.png'))\n",
    "# convert to gray + hsv\n",
    "img_gray = convert_rgb_to_grayscale(img)\n",
    "img_yuv= convert_rgb_to_yuv(img)\n",
    "gt_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "gt_yuv = cv.cvtColor(img, cv.COLOR_BGR2YUV)\n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 7))\n",
    "display_image(img, axes=ax[0][0])\n",
    "ax[0][0].set_title('Original')\n",
    "display_image(img_gray, axes=ax[0][1])\n",
    "ax[0][1].set_title('Grayscale')\n",
    "display_image(img_yuv, axes=ax[0][2])\n",
    "ax[0][2].set_title('YUV')\n",
    "\n",
    "display_image(img, axes=ax[1][0])\n",
    "ax[1][0].set_title('Original')\n",
    "display_image(gt_gray, axes=ax[1][1])\n",
    "ax[1][1].set_title('OpenCV - Grayscale')\n",
    "display_image(gt_yuv, axes=ax[1][2])\n",
    "ax[1][2].set_title('OpenCV - YUV');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to convert *BGR* image to a any color space, we can use this to extract a colored object. In *HSV*, it is more easier to represent a color than in *BGR* colorspace. In our application, we will try to extract a blue colored object. So here is the method:\n",
    "\n",
    "1. Reading an input image\n",
    "2. Convert from *BGR* to *HSV* color-space using bluit-in function from OpenCV\n",
    "3. We threshold the *HSV* image for a range of blue color\n",
    "4. Now extract the blue object alone, we can do whatever on that image we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_bgr_image(frame, lower, upper):\n",
    "    \"\"\"\n",
    "    Segment image.\n",
    "        1. Convert image to HSV color space\n",
    "        2. Binary masking using threshold values\n",
    "        3. Fuse mask\n",
    "    :param frame: BGR image to segment\n",
    "    :param lower: Lower threshold value\n",
    "    :param upper: Upper threshold value\n",
    "    :return: Segmented image, binary mask\n",
    "    \"\"\"\n",
    "    \n",
    "    # CODE HERE\n",
    "    \n",
    "    return res, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read input\n",
    "img = cv.imread(os.path.join('..','data', 'pencil.png'))\n",
    "\n",
    "# 2. Define range of blue color in HSV\n",
    "lower = np.asarray([90,120,120])\n",
    "upper = np.asarray([130,255,255])\n",
    "im, mask = threshold_bgr_image(img, lower, upper)\n",
    "\n",
    "# 3. Display using subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Input')\n",
    "display_image(mask, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title('Detected Mask')\n",
    "display_image(im, axes=ax[2])\n",
    "ax[2].set_title('Extracted color');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find *HSV* values, you can use the same function, `cv.cvtColor()`. Instead of passing an image, you just pass the *BGR* values you want. For example, to find the HSV value of Green, try following commands in Python terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = np.uint8([[[0, 0, 255 ]]])\n",
    "hsv_red = cv.cvtColor(red, cv.COLOR_BGR2HSV)\n",
    "print( 'HSV component {}'.format(hsv_red))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to extract the two **Yellow** colored pencils.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Code here \n",
    "# Extract YELLOW objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the grayscale image, try to extract the **BLACK** pencil. What are the pros and cons of each method (colors *vs* grayscale)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Code here \n",
    "# Extract BLACK object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Image Histogram\n",
    "\n",
    "So what is an histogram ? You can consider an histogram as a graph or plot, which gives you an overall idea about the intensity distribution of an image. It is a plot with pixel values (*ranging from 0 to 255 for 8-bits encoding, but it can be 16-bits, 24-bits, etc*) in X-axis and corresponding number of pixels in the image on Y-axis.\n",
    "\n",
    "It is just another way of understanding the image. By looking at the histogram of an image, you get intuition about contrast, brightness, intensity distribution etc of that image. Almost all image processing tools today, provides features on histogram.\n",
    "\n",
    "In the next cell, implement a function that compute the image histogram for all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_histogram(image):\n",
    "    \"\"\"\n",
    "    Compute image color distribution for each channels of a given `image`\n",
    "    :param image: Image to extract histograms for\n",
    "    :return: List of histogram, one per channel.\n",
    "    \"\"\"\n",
    "    hist = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'lena.png'))\n",
    "\n",
    "# Compute histogram for each channels\n",
    "hists = compute_image_histogram(img)\n",
    "\n",
    "# Plot \n",
    "color = ['b', 'g', 'r']\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
    "display_image(img, axes=ax[0])\n",
    "for k, hist in enumerate(hists):\n",
    "    ax[1].plot(hist, color=color[k])\n",
    "ax[1].set_title('Color Histogram by channels')\n",
    "ax[1].set_xlim([0, 256])\n",
    "ax[1].set_xlabel('Pixel Intensity')\n",
    "ax[1].set_ylabel('Number of Pixel')\n",
    "plt.legend(['Blue Channel', 'Green Channel', 'Red Channel'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the image and its histogram. Left region of histogram shows the amount of darker pixels in image and right region shows the amount of brighter pixels. \n",
    "\n",
    "From the histogram, you can see for the *green* channel that pixels are mainly dark and midtones (*i.e. pixel values in mid-range, say around 127*). There is no bright green pixels in the image. On the other hand, *red* channel is mainly composed of pixels with high intensity (*i.e. distribution mainly located on the right hand side*).\n",
    "\n",
    "#### 1.1.4.1 Histogram Equalization\n",
    "\n",
    "Consider an image whose pixel values are confined to some specific range of values only. For instance, brighter image will have all pixels confined to high values. But a good image will have pixels from all regions of the image. So you need to stretch this histogram to either ends and that is what Histogram Equalization does. This normally improves the contrast of the image ([doc](https://en.wikipedia.org/wiki/Histogram_equalization)).\n",
    "\n",
    "In the next section implement such contrast enhancement function (i.e. based on histogram equalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_enhancement(image):\n",
    "    \"\"\"\n",
    "    Compute histogram and apply histogram equalization on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :return: tuple: enhanced image, input image histogram, transformed image histogram\n",
    "    \"\"\"\n",
    "    \n",
    "    hist = None\n",
    "    hist_equ = None\n",
    "    image_equ = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return image_equ, hist, hist_equ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'hawkes_bay.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "# Enhance contrast with histogram equalization\n",
    "img_equ, hist, hist_equ = contrast_enhancement(img)\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 2, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0][0])\n",
    "ax[0][0].set_title('Original')\n",
    "# Histogram original\n",
    "ax[0][1].bar(np.arange(hist.shape[0]), hist.reshape(-1),  width=1)\n",
    "ax[0][1].set_xlim([0, 256])\n",
    "ax[0][1].set_title('Histogram')\n",
    "# Histogram equalized\n",
    "display_image(img_equ, axes=ax[1][0])\n",
    "ax[1][0].set_title('Histogram Equalized')\n",
    "ax[1][1].bar(np.arange(hist_equ.shape[0]), hist_equ.reshape(-1),  width=1)\n",
    "ax[1][1].set_xlim([0, 256])\n",
    "ax[1][1].set_title('Histogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How has the contrast evolved after applying the transformation ? \n",
    "- How has the histogram changed ?\n",
    "\n",
    "**QUESTION: In the next cell, comment on what you observe**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Image Thresholding\n",
    "\n",
    "Here, the matter is straightforward. If pixel value is greater than a threshold value, it is assigned one value (*maybe white*), else it is assigned another value (*maybe black*). The function used is `cv.threshold(<image>, <thresh>, <maxVal>, <type>)`. First argument is the source image, which should be a grayscale image. Second argument is the threshold value which is used to classify the pixel values. Third argument is the maxVal which represents the value to be given if pixel value is more than (sometimes less than) the threshold value. OpenCV provides different styles of thresholding and it is decided by the fourth parameter of the function. Different types are:\n",
    "\n",
    "1. `cv.THRESH_BINARY`\n",
    "2. `cv.THRESH_BINARY_INV`\n",
    "3. `cv.THRES_TRUNC`\n",
    "4. `cv.THRESH_TOZERO`\n",
    "5. `cv.THRESH_TOZERO_INV`\n",
    "\n",
    "In the next cell, implement these thresholding operations within the function  `threshold_image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_image(image, thresh):\n",
    "    \"\"\"\n",
    "    Apply different type of threshold to a given image + threshold value\n",
    "    \"\"\"\n",
    "    \n",
    "    img_bin = None\n",
    "    img_bin_inv = None\n",
    "    img_trunc = None\n",
    "    img_tozero = None\n",
    "    img_tozero_inv = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    return img_bin, img_bin_inv, img_trunc, img_tozero, img_tozero_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'grad.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Various threshold output\n",
    "thresh1, thresh2, thresh3, thresh4, thresh5 = threshold_image(img, 127)\n",
    "\n",
    "\n",
    "# Define how many figures will be drawn, save canvas + axes reference for later use\n",
    "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "fig, ax = plt.subplots(2, 3, figsize=(17, 9))\n",
    "# Draw each samples\n",
    "for k in range(6):\n",
    "    r = int(k / 3)\n",
    "    c = k % 3\n",
    "    # Show image\n",
    "    display_image(images[k], axes=ax[r][c], cmap='gray')\n",
    "    # Add title + remove tick\n",
    "    ax[r][c].set_title(titles[k])\n",
    "    ax[r][c].set_xticks([])\n",
    "    ax[r][c].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot multiple images, we have used `plt.subplots(...)` function. Please checkout Matplotlib docs for more details.\n",
    "\n",
    "#### 1.1.5.1 Otsu Binarization\n",
    "\n",
    "In global thresholding, we used an arbitrary value for threshold value. How can we know a value we selected is good or not? Answer is, trial and error method. But consider a bimodal image (In simple words, bimodal image is an image whose histogram has two peaks). For that image, we can approximately take a value in the middle of those peaks as threshold value. That is what Otsu binarization does. In simple words, it automatically calculates a threshold value from image histogram for a bimodal image. (For images which are not bimodal, binarization won’t be accurate.)\n",
    "\n",
    "For this, our `cv.threshold()` function is used, but pass an extra flag, `cv.THRES_OTSU`. For threshold value, simply pass zero. Then the algorithm finds the optimal threshold value and returns you as the second output, retVal. If Otsu thresholding is not used, retVal is same as the threshold value you used.\n",
    "\n",
    "\n",
    "Check out below example. Input image is a noisy image. In first case, a global thresholding value of 127 is applied. In second case, Otsu’s thresholding is applied directly. In third case, the image is filtered with a 5x5 gaussian kernel first to remove the noise, then applied Otsu thresholding. See how noise filtering improves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'noisy2.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# global thresholding\n",
    "ret1, th1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)\n",
    "# Otsu's thresholding\n",
    "ret2, th2 = cv.threshold(img, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv.GaussianBlur(img, (5,5), 0)\n",
    "ret3, th3 = cv.threshold(blur, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "\n",
    "# Compute histograms\n",
    "# docs: https://docs.opencv.org/3.4.0/d6/dc7/group__imgproc__hist.html#ga4b2b5fd75503ff9e6844cc4dcdaed35d\n",
    "hist_g = cv.calcHist([img], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "hist_f = cv.calcHist([blur], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "\n",
    "# plot all the images and their histograms\n",
    "images = [img, hist_g, th1,\n",
    "          img, hist_g, th2,\n",
    "          blur, hist_f, th3]\n",
    "titles = ['Original Noisy Image', 'Histogram', 'Global Thresholding (v=127)',\n",
    "          'Original Noisy Image', 'Histogram', \"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image', 'Histogram', \"Otsu's Thresholding\"]\n",
    "\n",
    "# Define how many figures will be drawn, save canvas + axes reference for later use\n",
    "fig, ax = plt.subplots(3, 3, figsize=(17, 9))\n",
    "# Draw each samples\n",
    "for k in range(9):\n",
    "    r = int(k / 3)\n",
    "    c = k % 3\n",
    "    if c != 1:\n",
    "        # Show image\n",
    "        display_image(images[k], axes=ax[r][c], cmap='gray')\n",
    "        # Add title + remove tick\n",
    "        ax[r][c].set_title(titles[k])\n",
    "    else:\n",
    "        # Draw histogram\n",
    "        ax[r][c].bar(np.arange(images[k].shape[0]), images[k].reshape(-1), width=1)\n",
    "        ax[r][c].set_title(titles[k])\n",
    "        ax[r][c].set_xlabel('Pixel value')\n",
    "        ax[r][c].set_ylabel('Pixel count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the global thresholding (case 1), why did we chose a value of 127? What would be the effect of a smaller threshold ?  Whats is the value of Otsu thresholding?\n",
    "- What would be the effect of a bigger Gaussian filter, for example 9x9 pixels? \n",
    "- In this eaxample, Otsu's thresholding gives very good results. Can you describe an example for which this method fails? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Introduction to Local Binary Pattern (*LBP*)\n",
    "\n",
    "(20 points)\n",
    "\n",
    "Local Binary Pattern operator transforms an image, usually grayscale, into an image of integer labels describing small-scale appearance. These labels or their statistics, most commonly the histogram, are then used for further image analysis. \n",
    "\n",
    "The *LBP* operator acts at pixel level and compares each pixels with their neighbours. The surrounding pixels are thresholded by its center pixel value, multiplied by powers of two and then summed to obtain a label for the center pixel. Figure below shows how the neighboring pixels are defined. For instance with a neighbourhood of 8 pixels, a total of $2^8$ different labels can be obtained depending on the relative gray values of the center and the pixels in the neighborhood.\n",
    "\n",
    "<img src=\"../data/lbp_sampling.png\" alt=\"Drawing\" style=\"width: 150px;\"/>\n",
    "\n",
    "Given a grayscale image $I\\left(x, y \\right)$ and $g_c$ being the gray level for an arbitrary pixel located at position $\\left(x, y \\right)$, then $g_c = I\\left(x, y \\right)$. Moreover assume $g_p$ being the gray value of a sampled point evenly spaced in the circular neighborhood of $P$ sampling points and a radius of $R$ around the point $\\left(x, y \\right)$, then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g_p &= I\\left(x_p, y_p \\right), \\quad p = 0, \\dots, P - 1 \\\\\n",
    "x_p &= x + R \\cos \\left(2 \\pi p/P \\right) \\\\\n",
    "y_p &= y - R \\sin \\left(2 \\pi p/P \\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As shown in the figure above, the sampling location might end up not corresponding to pixel position, therefore the intensity is estimated with interpolation. Most of the time we use `bilinear` interpolation [ref](https://en.wikipedia.org/wiki/Bilinear_interpolation). \n",
    "\n",
    "Finally the *LBP* operator can be defined as:\n",
    "\n",
    "$$ LBP_{P,R}\\left(x, y \\right) = \\sum_{p=0}^{P-1} s\\left(g_p - g_c \\right)2^{p} $$\n",
    "\n",
    "where the thresholding function $s(.)$ is defined as:\n",
    "\n",
    "$$\n",
    "s\\left(z\\right) = \\left\\{ \\begin{array}{ll} 1, & \\quad z \\geq 0 \\\\ 0, & \\quad z < 0 \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Once the *LBP* operator is applied on the image, the feature vector is computed by accumulating the labels into a histogram with $2^P$ bins. The figure below shows an example of the different steps involved into the computation of *LBP* feature vector.\n",
    "\n",
    "<img src=\"../data/lbp_example.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "More details can be found [here](https://link.springer.com/chapter/10.1007/978-0-85729-748-8_2).\n",
    "\n",
    "Task to perform:\n",
    "- Complete the implementation of the `bilinear` interpolation function\n",
    "- Complete the implementation of the *LBP* operator\n",
    "- Apply your operator on the `Lena` image using the code snippet provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_interpolation(image, x, y):\n",
    "    \"\"\"\n",
    "    Interpolate pixel value at position (`x`, `y`) on a given image.\n",
    "    \n",
    "    :param image: Image from where to interpolation (np array)\n",
    "    :param x: X coordinate of the location where to interpolate (float)\n",
    "    :param y: Y coordinate of the location where to interpolate (float)\n",
    "    :return: interpolated pixel value\n",
    "    \"\"\"\n",
    "    pix_value = 0.0\n",
    "    \n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    return pix_value\n",
    "\n",
    "def local_binary_pattern_operator(image, P, R):\n",
    "    \"\"\"\n",
    "    Ally Local Binary Pattern on a given image (Grayscale).\n",
    "    \n",
    "    :param image: Image to operate on\n",
    "    :param P: Number of circularly symmetric neighbour set points (quantization of the angular space).\n",
    "    :param R: Radius of circle (spatial resolution of the operator).\n",
    "    \"\"\"\n",
    "    assert(image.ndim == 2)\n",
    "    \n",
    "    \n",
    "    # Your code for LBP\n",
    "    \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'lena.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply LBP Operator\n",
    "lbp_img = local_binary_pattern_operator(img, 8, 1)\n",
    "\n",
    "# Compute normalized histogram\n",
    "lbp_hist = cv.calcHist([lbp_img.astype(np.float32)], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "lbp_hist /= lbp_hist.sum() + 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many figures will be drawn, save canvas + axes reference for later use\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Image')\n",
    "# Show LBP image\n",
    "display_image(lbp_img, axes=ax[1])\n",
    "ax[1].set_title('LBP Image')\n",
    "# Draw histogram\n",
    "ax[2].bar(np.arange(lbp_hist.shape[0]), lbp_hist.reshape(-1), width=1)\n",
    "ax[2].set_title('LBP Histogram')\n",
    "ax[2].set_xlabel('Pixel value')\n",
    "ax[2].set_ylabel('Pixel count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Introduction to Histogram of Gradient (*HoG*)\n",
    "\n",
    "(30 points)\n",
    "\n",
    "### 1.3.1 Introduction\n",
    "\n",
    "Histogram of Oriented Gradients (*HOGs*) are descriptors mainly used in computer vision and machine learning for object detection. HOG features were first introduced by Dalal and Triggs in their CVPR 2005 paper: \"Histogram of Oriented Gradients for Human Detection\" (<http://ieeexplore.ieee.org/document/1467360/>); which transforms image pixels into a vector representation that is sensitive to broadly informative image features regardless of confounding factors like illumination. Later on, we will use these descriptors for detection and tracking. \n",
    "\n",
    "\n",
    "In their work, Dalal and Triggs proposed HOG and a 5-stages descriptor to classify humans in static images.\n",
    "The 5 stages included:\n",
    "\n",
    "    1. Pre-processing and scaling.\n",
    "    2. Computing gradients in both the x and y directions.\n",
    "    3. Obtaining weighted votes in spatial cells (local histograms).\n",
    "    4. Contrast normalizing overlapping spatial cells (Blocks).\n",
    "    5. Collecting all Histograms of Oriented gradients to form the final feature vector.\n",
    "\n",
    "\n",
    "In most real-world applications, HOG is used in conjunction with a __Linear SVM__ to perform object detection. HOG rapidly became one of the most used descriptors in image classification. The reason HOG is employed so heavily is because the local object appearance and shape can be characterized using the distribution of local intensity gradients. \n",
    "\n",
    "We’ll be discussing the steps necessary to combine both HOG and a Linear SVM into an object classifier later in this course. But for now, just understand that HOG is mainly used as a descriptor for object detection and that later these descriptors can be fed into a machine learning classifier.\n",
    "\n",
    "\n",
    "### 1.3.2 Objectives of this module\n",
    "\n",
    "HOG based classifiers are already implemented in several methods inside OpenCV. In this module, we will follow the basic steps to construct a feature vector based on HOG from scratch. Once the basic concepts are clear, at the end of this notebook, we will include the OpenCV build-in function used in real-world applications. The parameters of this function should be then clear for you to tune and play with for later applications.\n",
    "\n",
    "*What are HOG descriptors used to describe?*\n",
    "\n",
    "HOG descriptors are mainly used to describe the structural shape and appearance of an object in an image, making them excellent descriptors for object classification. In addition, since HOG captures local intensity gradients and edge directions, it also makes them good as texture descriptors.\n",
    "\n",
    "\n",
    "### 1.3.3 Preprocessing\n",
    "\n",
    "Typically, a feature descriptor converts an image of size $ width \\times height \\times 3 $(channels) to a feature vector array of length $n$. In the case of our implementation of the HOG feature descriptor, the input image will be of size 64 x 128 x 3 and the output feature vector of a length that will depend on various choices that we will make along the notebook. To further simplify the code below, we will also reduce the channel dimentionality to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads base image\n",
    "img = cv.imread(os.path.join('..','data', 'lena.png'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# converts to grayscale\n",
    "bgr_holder = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "print('Image has dimensions: {}'.format(bgr_holder.shape))\n",
    "\n",
    "# resize the final image\n",
    "bgr_img= cv2.resize(bgr_holder, (64, 128))\n",
    "print('Image has dimensions: {}'.format(bgr_img.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Gradient computation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Image_gradient\n",
    "\n",
    "Gradient vectors (or “image gradients”) are one of the most fundamental concepts in computer vision; many vision algorithms involve computing gradient vectors for each pixel in an image. A gradient vector it’s simply a measure of the change in pixel values along the x-direction and the y-direction around each pixel. The magnitude is defined as:\n",
    "\n",
    "$$\n",
    "G(x,y) = \\sqrt{(\\Delta x^2 + \\Delta y^2)}\n",
    "$$\n",
    "\n",
    "and the phase: \n",
    "\n",
    "$$\n",
    "\\theta(x,y) = atan(\\frac{\\Delta x}{\\Delta y})\n",
    "$$\n",
    "\n",
    "Where: $\\Delta x = f(x+1,y) - f(x-1,y)$ and $\\Delta y = f(x,y+1) - f(x,y-1)$, are simply the directional change from one pixel to the other.\n",
    "\n",
    "The simplest implementation of the aforementioned operators computes each value by using a mask operator: $-1|0|1$ over the pixel position in each direction. However the Sobel operator is usually employed as it is more robust to intensity changes: https://en.wikipedia.org/wiki/Sobel_operator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV includes a build-in function for directional derivatives. \n",
    "# Calculate gradients gx, gy\n",
    "gx = cv.Sobel(bgr_img, cv2.CV_64F, 1, 0, ksize=1) # 1,0 means in the (1,0) direction\n",
    "gy = cv.Sobel(bgr_img, cv2.CV_64F, 0, 1, ksize=1)  # 0,1 means in the (1,0) direction\n",
    "\n",
    "## Rember to use the comand: \n",
    "##   help(cv.Sobel) \n",
    "## in case of troubles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sobel [function](https://docs.opencv.org/3.2.0/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d) is really useful for many applications where we would like to compute the local intensity changes, for example for edge detection and texture synthesis. \n",
    "\n",
    "The resulting objects from (gx, gy) contains the \"derivative map\" in the selected direction (1,0 for x; 0,1 for y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "obj = display_image(gx, axes=ax[0])\n",
    "ax[0].set_title('Derivative along X direction')\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[0])\n",
    "obj = display_image(gy, axes=ax[1])\n",
    "ax[1].set_title('Derivative along Y direction')\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zoom up a litte bit to see the intensity changes.\n",
    "#Region of interest\n",
    "gx_roi = gx[50:100, 30:50]\n",
    "gy_roi = gy[50:100, 30:50] #face region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "obj = display_image(gx_roi, axes=ax[0], cmap='viridis')\n",
    "ax[0].set_title('Derivative along X direction')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[0])\n",
    "obj = display_image(gy_roi, axes=ax[1], cmap='viridis')\n",
    "ax[1].set_title('Derivative along Y direction')\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION: Does the images above make sense?  Please provide some good captions to what it's being displayed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION: What's the meaning of the brightest or darkest pixels for each directions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images above shows the partial derivatives in a given direction. We can use them to compute the magnitude and phase as in the above formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude computation -> Gradient = sqrt(gx*gx + gy*gy)\n",
    "\n",
    "# OpenCv provides a build-in function to compute the magnitude matrix (image).\n",
    "G = cv.magnitude(gx,gy)\n",
    "\n",
    "# And the direction ( in degrees ) \n",
    "_,phase = cv.cartToPolar(gx, gy, angleInDegrees=True,)\n",
    "\n",
    "# We can actually compute the magnitude using the cartToPolar function. Try it! \n",
    "# ---------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Plots of the magnitude and the phase:\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 5))\n",
    "\n",
    "#The magnitude is shown in gray scale.\n",
    "obj = display_image(G, axes=ax[0])\n",
    "ax[0].set_title('Gradient Magnitude')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[0])\n",
    "\n",
    "# The phase is shown using a color map. \n",
    "# Is worth notice that there are many homogeneous regions.\n",
    "obj = display_image(phase, axes=ax[1], cmap='hsv')\n",
    "ax[1].set_title('Gradient Phase (in degrees)')\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image above, we can see than the phase is computed from 0 to 360 degrees. While this is conceptually correct,two pixels with very similar angular values towards the X-axis (let's say 0.1 radians and 6.2 radians) will be considered very different by a naive classifier. We can correct this behaviour by wrapping the phase from 0 to $\\pi$ (0 to 180 degrees). At the end of the notebook, you will be asked to correct the range of the phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the phase image above we can see that there are a a big number of pixels in red (values very close to zero). \n",
    "\n",
    "**QUESTION: Does this mean that the gradient is really well oriented in those regions, or can it be an different effect?**\n",
    "*Hint: consider the values of gx and gy in those regions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 Computation of the Histograms of the Phase for Local Cells\n",
    "\n",
    "As the name accurately suggest, the key step is to compute the histograms of Oriented Gradients (i.e. the phase). Those histograms however are computed locally for \"cells\" of a given size. By a cell we refer to a rectangular region of interest sliding across the image domain. The size and shape of the cell will affect the performance of HOG as a feature vector. Many variants have been proposed regarding the use of different cells sizes and multiple resolutions.\n",
    "\n",
    "\n",
    "We will focus in this exploration in fixed $8 px\\times 8px$ cells, since they fit the not-planned-at-all size of our input ( $128\\times 64$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is usually constant and it's never changed true the procedure.\n",
    "CELL_SIZE = 8;\n",
    "\n",
    "# Example of a cell drawn in yellow\n",
    "img_canvas = cv2.resize(img, (64, 128)) # temporal image.\n",
    "\n",
    "#useful for loop, can be done without it though.\n",
    "for stride_x in range(1,img.shape[0]-1):\n",
    "    for stride_y in range(1,img.shape[1]-1):\n",
    "        cv2.rectangle(img_canvas, (0 + (stride_x-1)*CELL_SIZE, 0 + (stride_y-1)*CELL_SIZE), (0 + stride_x*CELL_SIZE, 0 + stride_y*CELL_SIZE), (0, 255, 255))\n",
    "\n",
    "display_image(img_canvas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the Phase for the very first cell\n",
    "num_of_bins = 36 # this is an important parameter\n",
    "# Compute histogram\n",
    "hist = cv.calcHist([phase[0:8, 0:8].astype(np.float32)], channels=[0], mask=None, histSize=[num_of_bins], ranges=[0,360])\n",
    "# Plot\n",
    "plt.bar(np.arange(hist.shape[0]), hist.reshape(-1), width=1)\n",
    "plt.xlabel('Orientation')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Orientation distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image above shows the cell subdivisions of the (colour) image for cells of size $8\\times 8$. We then computed the histogram of gradient directions for each one of them. This will give us a consensus of the magnitude direction for a given fixed area. In order to compute the histogram, the number of bins considered will affect the performance and quality of the descriptor. In the original paper, Dalal and Triggs found that a 9 orientations histogram (from 0 to 180 degrees) performed well for human detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compute the histogram of each cell by following the same procedure that \n",
    "# we used for drawing the cells:\n",
    "\n",
    "\n",
    "def computeCellHistograms(phase_img, cell_size):\n",
    "    \"\"\"\n",
    "    Compute histogram for each cells and concatenate them.\n",
    "    \n",
    "    :param phase_img: 1 channel image with the gradient direction.\n",
    "    :params cell_size: custion cell size (8px) \n",
    "    :return: histogram\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resulting histogram vector\n",
    "    concatenated_histograms =[];\n",
    "    \n",
    "    # Number of cells in each direction.\n",
    "    x_num_blocks = int(phase_img.shape[0]/cell_size);\n",
    "    y_num_blocks = int(phase_img.shape[1]/cell_size);\n",
    "    \n",
    "    for stride_x in range(1,int(x_num_blocks)):\n",
    "        x_range = range((stride_x-1)*cell_size, stride_x*cell_size);\n",
    "        for stride_y in range(1,y_num_blocks):\n",
    "            y_range = range((stride_y-1)*cell_size, stride_y*cell_size);\n",
    "            hist_ = cv.calcHist([phase_img[x_range,y_range].astype(np.float32)], channels=[0], mask=None, histSize=[36], ranges=[0,360])\n",
    "            # Concatenate features\n",
    "            concatenated_histograms.append(hist_)\n",
    "            \n",
    "    return concatenated_histograms\n",
    "            \n",
    "# Compute histogram\n",
    "histograms = computeCellHistograms(phase, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION: What's the size of the concatenated histogram, does it makes sense, should we do something?**\n",
    "\n",
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above computes the histogram of each cell and concatenate it into a single array. However, the code above assumes that each direction has the same *importance* and contributes equally to the histogram. \n",
    "\n",
    "In the original paper, it was proposed to __weigth__ the angle histogram, for instance, using the magnitud or the normalized magnitude computed before. The effect is depicted bellow:\n",
    "\n",
    "<img src=\"https://gurus.pyimagesearch.com/wp-content/uploads/2015/03/hog_histogram_animation.gif\">\n",
    "<em> Image taken from: https://gurus.pyimagesearch.com </em>\n",
    "\n",
    "\n",
    "For each of the gradient blocks, the histogram bins are weighted by the gradient magnitude. The effect of normalizing such weights will directly affect the contribution of larger edges vs smaller features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CODING EXERCISE, modify the function *computeCellHistogram* above, the new function should use the gradient image G to weight the computed histogram.**\n",
    "\n",
    "HINT: The easiest way would be to write your own custom histogram function that takes a second image with the weights.\n",
    "\n",
    "Avoid using pre-defined functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCellHistograms(phase_img,cell_size):\n",
    "    \"\"\"\n",
    "    Compute weighted histogram for each cells and concatenate them.\n",
    "    \n",
    "    :param phase_img: 1 channel image with the gradient direction.\n",
    "    :params cell_size: custion cell size (8px) \n",
    "    :return: histogram\n",
    "    \"\"\"\n",
    "    weigthed_and_concatenated_histograms = []\n",
    "    \n",
    "    \n",
    "    # CODE HERE\n",
    "    \n",
    "    \n",
    "    return weigthed_and_concatenated_histograms;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Block normalization\n",
    "\n",
    "In the previous steps, we created a histogram based on the gradient of the image. Gradients of an image are sensitive to overall lighting. If you make the image darker by dividing all pixel values by 2 for example, the gradient magnitude will change by half, and therefore the histogram values will change by half. Ideally, we want our descriptor to be independent of lighting variations. In other words, we would like to “normalize” the histogram so they are not affected by lighting variations.\n",
    "\n",
    "The last step of the HOG algorithm takes overlapping blocks of cells to compute the final feature vector. Rather than normalizing each histogram individually, the cells are first grouped into __blocks__ and normalized based on all the histograms in the block. These procedure makes the feature detector less sensitive to intensity changes.\n",
    "\n",
    "\n",
    "<img src = \"https://gurus.pyimagesearch.com/wp-content/uploads/2015/03/hog_contrast_normalization.gif\">\n",
    "<em> Image taken from: https://gurus.pyimagesearch.com </em>\n",
    "\n",
    "Image above shows each block as a group of 4 cells. This is refered as a block of size 2 by 2 cells. Contrast normalization is performed over multiple overlapping cells. Using this scheme, each cell is represented in the histogram multiple times; however, this redundancy actually improves accuracy. Finally, after all blocks are normalized, we take the resulting histograms, concatenate them, and treat them as our final feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CODING EXERCISE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final function to compute the final feature vector.\n",
    "# Each block should be of size 4, i.e. 2 x 2 cells.\n",
    "# The normalization should be done via the L2 or L1.\n",
    "def BlockNormalizationAndFeatureConcatenation(histograms, x_size, y_size):\n",
    "    \n",
    "    #CODE HERE\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.7 Final Descriptor Size (CODING EXERCISE)\n",
    "\n",
    "The 64 x 128 pixels detection window will be divided into 7 blocks across and 15 blocks vertically, for a total of 105 blocks. Each block contains 4 cells with a 9-bins histogram for each cell, for a total of 36 values per block. This brings the final vector size to 7 blocks across $\\times$ 15 blocks vertically $\\times$ 4 cells per block $\\times$ 9-bins per histogram = 3,780 values.\n",
    "\n",
    "\n",
    "Confirm that the dimentions above coincide (spoiler alert: they don't) with these ones. If not, correct the proper lines of code to make the coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# YOUR ANSWER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.8 Feature Vector Visualization\n",
    "\n",
    "The final descriptor is then a 1-dimentional array with the concatenation of the blocks normalized histograms. However, this shouldn't prevent us from having an intuitive visualization of the computed features. \n",
    "\n",
    "**CODING EXERCISE: As last exercise, you need to display the feature vector over the image space (or any clever way). References bellow and the whole internet may have some good ideas about this. You are free to use any build-in function you may find.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.9 HOG in action\n",
    "\n",
    "As you may suspect already, OpenCV comes with a nice implementation of the HOG descriptor. Once you finish this notebook, you should be able to understand the basic parameters from the _cv.HOGDescriptor()_ custom class.\n",
    "\n",
    "After this module, we will make use of the build-in function in OpenCV to compute the HOG and some other classification features to perform detection and clasification. \n",
    "\n",
    "The small example above uses a simple Support Vector Machine algoritmh (SVM) to detect People in multi-scale (we are not limited to and custom image size) images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# load base image (check that we are not scaling, normalizing or changing the channels)\n",
    "img = cv.imread(os.path.join('..','data', 'person_104.bmp'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# The HOG detector returns an array with the Regions of maximum likehood to contain a human-shaped-form\n",
    "rects, weights = hog.detectMultiScale(img, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "persons = 0;\n",
    "for (x, y, w, h) in rects:\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, persons * 256), 2)\n",
    "    persons += 1;\n",
    "    \n",
    "display_image(img);\n",
    "plt.title('Detection results')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.9.1 A not so trivial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base image (check that we are not scaling, normalizing or changing the channels)\n",
    "img = cv.imread(os.path.join('..','data', 'person_454.bmp'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# The HOG detector returns an array with the Regions of maximum likehood to contain a human-shaped-form\n",
    "rects, weights = hog.detectMultiScale(img , winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "persons = 0;\n",
    "for (x, y, w, h) in rects:\n",
    "    cv2.rectangle(img , (x, y), (x + w, y + h), (0, 255, persons * 256), 2)\n",
    "    persons += 1;\n",
    "    \n",
    "display_image(img);\n",
    "plt.title('Detection results')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.10 References\n",
    "\n",
    "https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
    "\n",
    "https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "\n",
    "http://juliaimages.github.io/ImageFeatures.jl/latest/tutorials/object_detection.html\n",
    "\n",
    "https://www.learnopencv.com/tag/hog/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Introduction to Scale Invariant Feature Transform (*SIFT*)\n",
    "\n",
    "### 1.4.1 Introduction\n",
    "\n",
    "Good feature descriptor should be resilient to various geometric transformations such as *translsation*, *rotation* and *scaling*. For example, check a simple image below. A corner in a small image within a small window (*i.e. left side*) is flat when it is scaled up in the same window.\n",
    "\n",
    "<img src=\"../data/corner.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "In 2004, **D.Lowe**, University of British Columbia, came up with a new feature descriptor, Scale Invariant Feature Transform (*SIFT*) in his paper, Distinctive Image Features from Scale-Invariant Keypoints, which extract keypoints and compute its descriptors ([ref](https://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/lowe_ijcv2004.pdf)).\n",
    "\n",
    "In order to achieve such invariance, there are four steps involved in *SIFT*:\n",
    "- Detect extremums in Scale-space\n",
    "- Localize keypoints\n",
    "- Define Keypoint's orientation\n",
    "- Build Keypoint descriptor\n",
    "\n",
    "### 1.4.2 Detect extremums in Scale-space\n",
    "\n",
    "From the example above, it is clear that we can not use one single window to detect keypoints with different scales. Therefore, ***scale-space*** filtering is used. In the filtering process, Laplacian of Gaussian (*LoG*) is computed for the image with various $\\sigma$ values. The LoG act as a *blob* detector where $\\sigma$ defines the \"size\" / \"scale\" of the blob to detect. With different values of $\\sigma$ we can find **local** maxima across the scale and the position in the image which give a list of $\\left(x, y, \\sigma \\right)$ values that are potential keypoint $\\left( x, y\\right)$ at scale $\\sigma$.\n",
    "\n",
    "The problem with this approach is that *LoG* is costly to compute. Therefore *SIFT* uses Differences of Gaussians (*DoG*) which is an approximation of *LoG*. *Dog* is computed as the difference between an image blurred with two different $\\sigma$ value, one being $\\sigma$ and the second one being $k \\sigma$.  This process is done for different octaves of the image in a Gaussian Pyramid as shown in the image below:\n",
    "\n",
    "<img src=\"../data/dog.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Once *DoG* is computed, every layers are searched for local extrema over scale and space. An extrema is defined by comparing the pixel value with its 8 respective neighbours and the 9 neighbours from the previous and next scales as shown in the figure below:\n",
    "\n",
    "<img src=\"../data/local_extrema.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "If it is a local extrema, potentially it is a keypoint. It means that keypoint is best represented in that scale. Regarding the parameters, the paper gives some empirical values; number of octaves = 4, number of scale levels = 5, initial $\\sigma=1.6$, $k=\\sqrt{2}$ as optimal values.\n",
    "\n",
    "### 1.4.3 Localize keypoints\n",
    "\n",
    "With potential keypoint locations, they need to be refined in order to increate the accuracy / robustness. Taylor expansion of the scale-space is used to have a better localization of the extrema and if its intensity is larger than a given treshold, the candidate is kept, otherwise it is discarded.\n",
    "\n",
    "Edges in *Dog* pyramid will have large responses, therefore they need to be removed. To do so, the local curvature is computed using a 2x2 Hessian matrix. For edges, the ratio between the two eigenvalues of this matrix will be large, therefore if the ratio is larger than a given threshold, the candidate will be assumed to be an edge and be discarded.\n",
    "\n",
    "After this step, low contrast position and edges will be discarded. Only the strong keypoints of interest remains.\n",
    "\n",
    "### 1.4.4 Define orientation\n",
    "\n",
    "So far keypoints have been treated to be rebust against scaling, now the rotation invariance need to be achieved. In a neigbourhood around the keypoint location with dimension depending on the scale, the gradient magnitude and direction is computed. An orientation histogram composed of 36 bins spanning $360^{\\circ}$ is created, with each bin weighted by its gradient magnitude. The highest peak in the histogram is taken as keypoint's orientation. \n",
    "\n",
    "\n",
    "### 1.4.5 Build descriptor\n",
    "\n",
    "Last step consists of building the keypoint's descriptor. To do so, a 16x16 neighbourhood is taken. The region is itself divided again into 16 sub-blocks of size 4x4. For each sub-blocks, an orientation histogram of 8 bins is computed. Finally, histogram from each sub-blocks are concatenated to form the final feature vector of dimension $8 \\times 16 = 128$.\n",
    "\n",
    "### 1.4.6 Your task\n",
    "\n",
    "Given a list of [cv::keypoints](https://docs.opencv.org/3.2.0/d2/d29/classcv_1_1KeyPoint.html#a4484e94502486930e94e7391adf9d215) we ask you to implement below the function that will generate the visualization for each keypoints. We want to display the magnitude dans the orientation of each keypoints at their location $\\left(x, y\\right)$. The magnitude can be represented by a circle with its radius. The orientation on the other hand can be an axis going from the circle's center to the rim with proprer angle.\n",
    "\n",
    "Check the documentation, for more information about how to draw [parts](https://docs.opencv.org/3.2.0/d6/d6e/group__imgproc__draw.html#ga7078a9fae8c7e7d13d24dac2520ae4a2) with opencv ([circle](https://docs.opencv.org/3.2.0/d6/d6e/group__imgproc__draw.html#gaf10604b069374903dbd0f0488cb43670), [line](https://docs.opencv.org/3.2.0/d6/d6e/group__imgproc__draw.html#ga7078a9fae8c7e7d13d24dac2520ae4a2), ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoint(img, kps):\n",
    "    \"\"\"\n",
    "    Draw SIFT keypoints on a given image. \n",
    "    \n",
    "    :param img: Grayscale image from which SIFT features have been extracted\n",
    "    :param kps: List of keypoints detected\n",
    "    :return: Color image with magnitude and orientation of each detected keypoints \n",
    "             (Magnitude is shown as a circle and orientation has an axis going from the center of the circle towards \n",
    "             the rim with proper orientation)\n",
    "    \"\"\"\n",
    "    # Convert Grayscale image to color BGR\n",
    "    canvas = \n",
    "    # Draw keypoints\n",
    "    \n",
    "    \n",
    "    # Return canvas\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'lena.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create sift extractor and detect keypoints\n",
    "sift = cv.SIFT_create()\n",
    "kps = sift.detect(img, None)\n",
    "\n",
    "# Draw\n",
    "canvas = draw_keypoint(img, kps)\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(1, 2, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Image')\n",
    "display_image(canvas, axes=ax[1])\n",
    "ax[1].set_title('SIFT Keypoints extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
