{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student's name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aim of this course is to review the evolution of image processing tools from hand-crafted methods to deep learning algorithms. The semester is split into four labs :\n",
    "\n",
    "* **Lab 1** : Introduction to Image Processing Using Hand-Crafted Features\n",
    "* **Lab 2** : Object detection\n",
    "* **Lab 3** : Object tracking\n",
    "* **Lab 4** : Introduction to Deep Learning for image classification and generative model\n",
    "\n",
    "Let's start with the second chapter of this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 : Object Detection and Recognition\n",
    "(*100 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn imageio\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import tarfile\n",
    "from utils import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Template Matching\n",
    "(*40 points*)\n",
    "\n",
    "In this warm-up section, we will address the problem of detection and recognition using Template Matching. \n",
    "\n",
    "Template matching is a 'brute-force' algorithm for object recognition. The most basic method of template matching is to directly compare the grayscale images, without using edge detection. For example, if you were trying to detect, let's say a football, you will need to create a base template of the object. During the operation, the template matching algorithm would analyze the current image to find areas which are similar to the template. This basic approach is quite limited. For one thing, it is not robust to inconsistent changes in brightness within the image. If the template image has strong features, a feature-based approach may be considered; the approach may prove further useful if the match in the search image might be transformed in some fashion. For templates without strong features, or for when the bulk of the template image constitutes the matching image, a template-based approach may be effective. \n",
    "\n",
    "In the naive approach, the difference between the template and the matching area is computed pixel by pixel and used to calculate the overall error. It is possible to reduce the number of sampling points by reducing the resolution of the search and template images by the some factor and performing the operation on the resultant downsized images (multiresolution, or Pyramid (image processing)), providing a search window of data points within the search image so that the template does not have to search every viable data point or a combination of both.\n",
    "\n",
    "![Template Matching Sample](../data/templateMatch.jpeg)\n",
    "\n",
    "Template matching example. Left: Template image. Right: Input image with the resulting image highlighted.  \n",
    "\n",
    "### 2.1.1 Objectives\n",
    "\n",
    "In this section, we will explore the advantages and disadvantages of template matching method. However, in contrast with the previous Chapter, we will leave most of the implementation for the reader (i.e. you).\n",
    "\n",
    "The following section will introduce to the most common metrics used for the matching distance and how to are used in OpenCV. Your task will be to: \n",
    " * Implement each metric *by hand* \n",
    " * Compare the accuracy against the OpenCV method \n",
    " * Analyse and report your observations for each metric in 3 exercises and one mini-challenge.\n",
    "\n",
    "As the final exercise, you will be given a set of more \"challenging\" data examples where using what you *learned before*, you will be asked to detect several objects in the scene. Your resulting algorithm __should have the given set of inputs and outputs__.\n",
    "\n",
    "### 2.1.2 Distance, Minimums and Maximums\n",
    "\n",
    "The two (and pretty much only) important parts of the Naive Template Matching algorithm is the *distance transform*, i.e. the metric to know if we found a match or not, and the global minima detection. \n",
    "\n",
    "For an  Input image $I$ if size $W\\times H$, a template Image $T$ of size $w\\times h$; ($w<W, h<H$), the distance methods implemented in OpenCv are the following: \n",
    "\n",
    "* Mean Squared Difference Method=CV_TM_SQDIFF\n",
    "\n",
    "\\begin{equation*}\n",
    "R(x,y)= \\sum _{x',y'} (T(x',y')-I(x+x',y+y'))^2 \n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized Mean Squared Difference Method=CV_TM_SQDIFF_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{\\sum_{x',y'} (T(x',y')-I(x+x',y+y'))^2}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "* Cross Correlation Method=CV_TM_CCORR\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\sum _{x',y'} (T(x',y') \\cdot I(x+x',y+y'))\n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized Cross Correlation method=CV_TM_CCORR_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{\\sum_{x',y'} (T(x',y') \\cdot I(x+x',y+y'))}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "* Correlation Coefficient Method=CV_TM_CCOEFF\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\sum _{x',y'} (T'(x',y') \\cdot I'(x+x',y+y'))\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{array}{l} T'(x',y')=T(x',y') - 1/(w \\cdot h) \\cdot \\sum _{x'',y''} T(x'',y'') \\\\ I'(x+x',y+y')=I(x+x',y+y') - 1/(w \\cdot h) \\cdot \\sum _{x'',y''} I(x+x'',y+y'') \n",
    "\\end{array}\n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized Correlation Coefficient=CV_TM_CCOEFF_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{ \\sum_{x',y'} (T'(x',y') \\cdot I'(x+x',y+y')) }{ \\sqrt{\\sum_{x',y'}T'(x',y')^2 \\cdot \\sum_{x',y'} I'(x+x',y+y')^2} }\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Please notice that the dimensions of the output image, $R$, will depend on how you handle the edges. The easiest way is to ser the return an output image of size $(W-w+1, H-h+1)$.\n",
    "\n",
    "After the function finishes the comparison, the resulting image will contain an image map with the obtained values. In OpenCV, the best matches can be found as global minimums or maximums (depending which matric you used) using the `minMaxLoc()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.1 Exercise\n",
    "\n",
    "From the functions listed above, indicate if the best matching position is located either in the local minimums or in the maximums. \n",
    "\n",
    "**Write your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 Exercise\n",
    "\n",
    "Implement at least 3 of the functions listed above, and use them as arguments in the base method provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE, You're distance function\n",
    "# ...\n",
    "\n",
    "def my_distance_fn(image_patch, template):\n",
    "    return  0.0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = cv.imread('../data/space-invaders_1.jpg', 0)\n",
    "template = cv.imread('../data/template_0.png', 0)\n",
    "\n",
    "def template_matching_process(image, template, dist_fn):\n",
    "    \"\"\"\n",
    "    Given an input image, iterates over the image and computes the distance w/r\n",
    "    the template, using a given distance function. \n",
    "\n",
    "    :input_image:       Input image. :) \n",
    "    :template:          The Template Image.\n",
    "    :distance_function  Function used to compute the distance. The function should receive a image patch \n",
    "                        and a template as inputs.\n",
    "    :return:            The distance map.\n",
    "    \"\"\"\n",
    "    W = image.shape[1];\n",
    "    H = image.shape[0];\n",
    "    \n",
    "    w = template.shape[1];\n",
    "    h = template.shape[0];\n",
    "\n",
    "    # Output image/map\n",
    "    dist_map = np.zeros((H-h+1,W-w+1), dtype=np.float32)\n",
    "\n",
    "    # we could avoid the fors by using lambda funnctions.\n",
    "    for y in range(dist_map.shape[0]):\n",
    "        for x in range(dist_map.shape[1]):\n",
    "            # We take just the sub-patch where to compute the distance\n",
    "            holder_patch = image[y:y+h, x:x+w];\n",
    "            # for each point we compute the distance w/r the template\n",
    "            dist_map[y, x]= dist_fn(holder_patch, template); \n",
    "    return dist_map\n",
    "\n",
    "# Use it like this\n",
    "dist_map = template_matching_process(img_gray, template, my_distance_fn)\n",
    "\n",
    "plt.figure(figsize=(12.8, 8.2))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img_gray, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(template, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Template')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(dist_map)\n",
    "plt.axis('off')\n",
    "plt.title('Distance Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.3 Exercise\n",
    "\n",
    "Check your outputs by using the build-in functions in OpenCV to compute the maps for each of the methods implemented. Show in each cell: your map and the OpenCV map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "res = cv.matchTemplate(img_gray, template, cv.TM_SQDIFF)\n",
    "\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7),squeeze=False)\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"My Distance Map\")\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv Distance Map\")\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# ....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, the self-implemented method is quite slow even for small images, since it increases quadratically for the input image and also the template image. OpenCV implements the same function optimally (low-level implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Finding Local Minimums / Maximums\n",
    "\n",
    "As explained before, in order to find the location of our possible object we need to find the minimum or maximum point in our resulting distance map. Using OpenCV, the function `minMaxLoc()` can be used to find the local minimum and maximum of the single-channel array (1D or 2D) [[doc](https://docs.opencv.org/4.0.0/d2/de8/group__core__array.html#gab473bf2eb6d14ff97e89b355dac20707)]. \n",
    "\n",
    "However, if we happen to have several objects in the image that we would like to match `cv.minMaxLoc()` won't give you all the locations. \n",
    "\n",
    "#### 2.1.3.1 Exercise\n",
    "\n",
    "Write your own function, `multiMinMax(src, flag, params)`, which should take an input 2D image `src` and return an `output_array` with the local minimums or maximums depending on the provided `flag` (`flag = \"min\"` or `flag = \"max\"`), and a given `params`. The `params` can be, for example, a _threshold_ for the local minima/maxima, the maximum number of maximums/minimums to return, a difference between the global maxima/minima to be included, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiMinMax(src, flag, params):\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    # ...\n",
    "                \n",
    "    return (np.asarray([0]), np.asarray([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawROIS(src, template, locations, color = (0, 0, 255)):\n",
    "    w, h = template.shape[::-1]\n",
    "    # Make copy of image to draw on it without changing the original image\n",
    "    canvas = src.copy();\n",
    "    \n",
    "    # Draw all rectangles\n",
    "    for pt in zip(*locations[::-1]):\n",
    "        cv.rectangle(canvas, pt, (pt[0] + w, pt[1] + h), color, 2)\n",
    "    \n",
    "    return canvas\n",
    "\n",
    "# How should be called:\n",
    "img_rgb  = cv.imread('../data/space-invaders_1.jpg')\n",
    "img_gray = cv.imread('../data/space-invaders_1.jpg',0)\n",
    "template = cv.imread('../data/template_0.png',0)\n",
    "\n",
    "# For a given distance Map\n",
    "distance_map_norm_corr = cv.matchTemplate(img_gray, template, cv.TM_CCORR_NORMED)\n",
    "# Use your function here!\n",
    "locations = multiMinMax(distance_map_norm_corr, 'max', None)\n",
    "# Draw the ROIs \n",
    "img_rgb_holder = drawROIS(img_rgb, template, locations)\n",
    "# example:\n",
    "display_image(img_rgb_holder)\n",
    "# How many ROIS did you returned? \n",
    "print(np.array(locations).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.2 Exercise\n",
    "\n",
    "Now that you are all set up, use the functions above and *try* to detect ALL the matching objects in the input image (Using template Matching).\n",
    "\n",
    "Rules:\n",
    "\n",
    "* You can use any metric you want, self-implemented or from OpenCV. \n",
    "* You can tweak your multiMinMax to get better results.\n",
    "* For 1) and 2) and 3) you have to do it in grayscale.\n",
    "* 4) can use multi-channel heuristics.\n",
    "\n",
    "\n",
    "#### 1) Perfect match <3\n",
    "\n",
    "Using the base input provided, plot the input image _showing_ the locations of the matching objects for the 2 provided templates.\n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* Why did you choose that given metric?  \n",
    "* How robust to false positives/negatives is your selected metric.\n",
    "* Is the number of output locations the same as the matching objects? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input images to use\n",
    "p1_src_rgb    = cv.imread('../data/space-invaders_1.jpg')\n",
    "p1_src_gray   = cv.imread('../data/space-invaders_1.jpg',0)\n",
    "\n",
    "# Show the matching of these 2 templates:\n",
    "p1_template_1 = cv.imread('../data/template_1.png',0)\n",
    "p1_template_2 = cv.imread('../data/template_2.png',0)\n",
    "\n",
    "# Image\n",
    "display_image(p1_src_gray)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(3, 1),squeeze=False)\n",
    "\n",
    "# Templates\n",
    "display_image(p1_template_1, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Template 1\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "display_image(p1_template_2, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"Template 2\")\n",
    "ax[0][1].set_xticks([])\n",
    "ax[0][1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# .....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Not so perfect Match </3\n",
    "\n",
    "Plot the input image _showing_ the locations of the matching objects and any false positive. All the _invaders_ in the same row counts as the \"same\" class. \n",
    "\n",
    "Follow the code bellow and provide some insights like:\n",
    "\n",
    "* How did you select the number of maximums/minimus?  \n",
    "* How robust to false positives/negatives is your selected metric.\n",
    "* Is the number of output locations the same as the matching objects? \n",
    "* Could you use any of the features from the last chapter to improve the matching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "p2_src_rgb    = cv.imread('../data/space-invaders_2.jpg')\n",
    "p2_src_gray   = cv.imread('../data/space-invaders_2.jpg',0)\n",
    "\n",
    "# 2 tempaltes (check that tempalte one don't match all the invaders in the same row) \n",
    "p2_template_1 = cv.imread('../data/template_3.png',0)\n",
    "p2_template_2 = cv.imread('../data/template_1.png',0)\n",
    "\n",
    "# Image\n",
    "display_image(p2_src_gray)\n",
    "\n",
    "# Templates\n",
    "fig, ax = plt.subplots(1, 2, figsize=(3, 1),squeeze=False)\n",
    "display_image(p2_template_1, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Template 1\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "display_image(p2_template_2, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"Template 2\")\n",
    "ax[0][1].set_xticks([])\n",
    "ax[0][1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# .....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Hidden objects game\n",
    "\n",
    "\n",
    "Finding hidden objects in cluttered illustration is a popular casual game that develops your observation skills. In this exercise, you will use your template matching skills to establish an efficient object detector for this game.\n",
    "\n",
    "\n",
    "We will play with an example from the **Big Home Hidden Objects** game, where images to find are shown at the bottom. We provide you the method `extract_big_home_templates` to extract the grayscaled template of each object from the image.\n",
    "\n",
    "Your task is to use these templates and what you have learned to find the hidden objects in the given illustration.\n",
    "\n",
    "Plot the input image _showing_ the location of each object and _report_ the number of misses (we expect at most 5). Note each object is present once in the scene. So you can write a variant of `multiMinMax` (e.g `minMax`) which returns the global minimum or maximum of a 2D `src` image according to the provided `flag` (`flag = \"min\"` or `flag = \"max\"`). You can reuse the function `cv.minMaxLoc()`.\n",
    "\n",
    "\n",
    "In this exercise, you may choose to use any transformation in the input image (like the scale to save time) or tweak the distance metric. You can't modify the templates (only scale it).\n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* What metric seemed to work better this time? \n",
    "* Was it different from the previous exercise?\n",
    "* How many objects did you find? Why are these in particular?\n",
    "\n",
    "Save the output in a separate image for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS FUNCTION\n",
    "def extract_big_home_templates(image):\n",
    "    \n",
    "    # We extract list of objects in a heuristic manner\n",
    "    list_image = image[370:418, 77:712].copy()\n",
    "    \n",
    "    # There are 10 objects per image\n",
    "    n_objects = 10\n",
    "    stride = list_image.shape[1]/10\n",
    "    \n",
    "    # Extract each template image\n",
    "    objects = []\n",
    "    for i in range(n_objects):\n",
    "        object_rgb = list_image[:,int(stride*i):int(stride*(i+1))][15:-15,15:-15]\n",
    "        center_coordinates = (int(object_rgb.shape[1]/2), int(object_rgb.shape[0]/2))\n",
    "        object_rgb = cv2.ellipse(object_rgb, center_coordinates, (40, 32), 0, 0, 360, (35, 35,35), 20)\n",
    "        object_gray = cv2.cvtColor(object_rgb, cv2.COLOR_BGR2GRAY)\n",
    "        objects.append(object_gray)\n",
    "        \n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "hidden_rgb  = cv.imread('../data/hidden.jpeg')\n",
    "\n",
    "# get list of templates\n",
    "templates = extract_big_home_templates(hidden_rgb)\n",
    "hidden_rgb = hidden_rgb[80:360]\n",
    "hidden_gray = cv2.cvtColor(hidden_rgb, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 10),squeeze=False)\n",
    "\n",
    "display_image(hidden_rgb, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Input\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,10, figsize=(20, 20),squeeze=False)\n",
    "\n",
    "for i, template in enumerate(templates):\n",
    "    display_image(template, axes=ax[0][i])\n",
    "    ax[0][i].set_title(\"Templates {:d}\".format(i+1))\n",
    "    ax[0][i].set_xticks([])\n",
    "    ax[0][i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMax(src, flag):\n",
    "    ### Your code below\n",
    "    #\n",
    "    location = (0,0)\n",
    "    \n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Find all hidden objects\n",
    "\n",
    "As you can see, using template matching can be tricky, even when you have _good_ templates. As for the last exercise, you will try to design good __templates__  (it can be done in Paint if you want =P) in order to find all hidden objects in the image. In this exercise, you can (if you wish to) use the information of the 3 RGB channels (to generate a _better_ 1 channel image, for example) and transform the input image (for example, to homogenize the scale!). You can reuse any features from the last chapter to improve the matching.\n",
    "Plot your selected templates and the input image _showing_ the location of each object (if you can) and any miss, if any.\n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* Was it different from the previous exercise?\n",
    "* What did you improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "hidden_rgb  = cv.imread('../data/hidden.jpeg')\n",
    "\n",
    "# get list of templates\n",
    "hidden_rgb = hidden_rgb[80:360]\n",
    "hidden_gray = cv2.cvtColor(hidden_rgb, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 10),squeeze=False)\n",
    "\n",
    "display_image(hidden_rgb, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Input\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,10, figsize=(20, 20),squeeze=False)\n",
    "\n",
    "# YOUR CODE HERE TO LOAD YOUR TEMPLATES\n",
    "templates = []\n",
    "\n",
    "for i, template in enumerate(templates):\n",
    "    display_image(template, axes=ax[0][i])\n",
    "    ax[0][i].set_title(\"Templates {:d}\".format(i+1))\n",
    "    ax[0][i].set_xticks([])\n",
    "    ax[0][i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Person Detection\n",
    "(*30 points*)\n",
    "\n",
    "In this section, we will return to the HOG features from the last Chapter. As we said before, HOG was proposed as a useful feature for human detection. If you reach this point, you may have noticed that Template matching may not be the best option for this. Imagine how difficult it would be to create a template for any human-shaped structure that you would like to detect as a human in a scene. Instead of that, you will train a Linear Classifier from scratch.\n",
    "Section objectives:\n",
    "In this section, since you know the basics of HOG, you will use OpenCV's implementation to extract the HOG's features of the curated INRIA's Persons dataset to train an SVM Linear classifier (https://en.wikipedia.org/wiki/Support_vector_machine). For this, instead of using OpenCV's (already trained) classifier, we will use the Scikit-learn Machine Learning library (http://scikit-learn.org). Which is one of the most used machine learning libraries around this days.\n",
    "\n",
    "### 2.2.1 Dataset \n",
    "\n",
    " This dataset was collected as part of the research work on detection of upright people in images and video. The research is described in detail in the CVPR 2005 paper _Histograms of Oriented Gradients for Human Detection_. The full dataset is about ~1 GB and contains several thousands of pedestrian images. \n",
    " \n",
    "For your convenience, the dataset is already separated into two sets: \n",
    "* \"**_Positives_**\" which are all the images containing at least one person. \n",
    "* \"**_Negatives_**\" any kind of non-human shaped objects images.\n",
    "\n",
    "In addition, the data is already separated in a **training** and **testing** set (seriously, it cannot be more conveniently done).\n",
    "\n",
    "You can download the dataset using this [link](ftp://ftp.inrialpes.fr/pub/lear/douze/data/INRIAPerson.tar) and put the `.tar` file into the `data` folder. To uncompress the data, you can either use the function below or directly do it in your file explorer, it is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_extract(filename, force=False):\n",
    "    \"\"\"\n",
    "    Uncompress a given *.tar file\n",
    "    :param filename: File to be uncompressed\n",
    "    \"\"\"\n",
    "    # remove .tar.gz\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0] \n",
    "    if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s dataset (seems to be) already present.\\nSkipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        tar.extractall(os.path.dirname(filename))\n",
    "        tar.close()\n",
    "    print(\"All setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "maybe_extract(filename='../data/INRIAPerson.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Features Extraction\n",
    "\n",
    "Once you have the data, you will now process each image on both: _positive_ and _negative_ folders using the OpenCV HOG Descriptor implementation [[doc](https://docs.opencv.org/4.0.0/d5/d33/structcv_1_1HOGDescriptor.html)]: \n",
    "\n",
    "Your job: Using the skeleton provided below, for each image in the \"pos\" and \"neg\" folder of the *training* set:\n",
    "\n",
    "* Compute the hog feature vector using the parameters provided below. \n",
    "    * The length of each individual feature vector should be of 16800. Derive bellow why is of this size. (Hint: Imagine that you have an image of size (32,32) which would be the size of this case?).\n",
    "   \n",
    "* Append the feature vector to the _training_feature_ list.\n",
    "    * The total size of this list should be of (16800, number of images in your training set). \n",
    "\n",
    "* For each image add to the _label_ vector a 1 if it's positive or 0 if it's negative. The final length of the _labels_ should be the number of images in your full training set.\n",
    "\n",
    "**WARNING**: The dataset may contain corrupted images. Be sure, inside your code, to check if the image was loaded properly. Otherwise, you will get either trash features or execution errors.\n",
    "\n",
    "Previously, some student had issues loading the images. That's why we've provided four text files (i.e. `inria_{train, test}_{pos, neg}.txt`) to solve the issue. Each line in the file indicates the path to an image. Moreover sometimes OpenCV can not load the image properly, that's why we're gonna use another package to load them. The command below shows how to achieve the same as `cv2.imread(...)` function.\n",
    "\n",
    "```python\n",
    "np.flip(np.asarray(imageio.imread('<image_path>', dtype=np.uint8))[..., :3], -1)\n",
    "```\n",
    "\n",
    "The `imread(...)` function load the image, it is then converted to numpy array with `np.asarray()`. Only the first 3 channels are selected with `[..., :3]` and finally it is converted from `RGB` to `BGR` with the `np.flip(..., -1)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "#Positive folder:\n",
    "pos_im_path = \"../data/inria_train_pos.txt\"\n",
    "#Negative folder:\n",
    "neg_im_path = \"../data/inria_train_neg.txt\"\n",
    "\n",
    "\n",
    "# Image descriptor parameters\n",
    "# ---------------------------\n",
    "# Window size: this specifies the size of the input image (remember to scale the input to this size!)\n",
    "win_size     = (64,128)\n",
    "# Size on pixels of each block (remember that a block contains a set of CELLS)\n",
    "block_size   = (16,16)\n",
    "# The separation between each block. If this value is less than the block size, \n",
    "# there will be overlapping blocks. \n",
    "block_stride = (8,8)\n",
    "# The size of each CELL. Each cell computes one histogram. The cells should FIT inside a block.\n",
    "cell_size = (4,4)\n",
    "# Number of bins for each histogram.\n",
    "nbins = 10\n",
    "\n",
    "# Samples\n",
    "# ---------------------------\n",
    "# list to save ALL the features.\n",
    "training_features = []\n",
    "# Auxiliary array to label each features if it comes from a \"positive\"(1) or \"negative\" (0) image.\n",
    "labels = []\n",
    "\n",
    "print (\"Calculating the descriptors for the positive samples...\")\n",
    "\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# .....\n",
    "    \n",
    "    \n",
    "print (\"Calculating the descriptors for the negative samples...\")\n",
    "## YOUR CODE HERE\n",
    "# .....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Classifier\n",
    "\n",
    "Now that you have this features computed and saved in a feature matrix, you are pretty much set to train a classifier. \n",
    "\n",
    "scikit-learn provides all the implementation needed to train a linear classifier with no more than 2 lines of code! (Which is awesome and sad at the same time). \n",
    "\n",
    "For this exercise, you will use a Linear Support Vector Classifier [[doc](\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)].\n",
    "\n",
    "Please take the time to read the implementation details, but more importantly, the examples and theory provided in the documentation. If you are not interested in knowing stuff, you can jump right away to use the implemented functions in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Evaluation\n",
    "\n",
    "After training the classifier -which a fancy way to say that you fitted a 1-D vector of coefficients!- you can use this Support Vector Machine to *classify* if, given an input * HOG feature vector* (with strictly the same dimensions as your training data), it came from an image with a human-shaped form in it (prediction = 1), or not (prediction = 1).\n",
    "\n",
    "*Your job*:\n",
    "\n",
    "For each image in the test folders: \n",
    "* Compute the HOG feature vector.\n",
    "* Predict/classify the vector as positive (1) or negative (0); *Hint: LinearSVC.predict(...)*\n",
    "* Compute the estimation error for the negative and positive images _separetely_.\n",
    "* Compute and report the F1-score [[doc](https://en.wikipedia.org/wiki/F1_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# ....\n",
    "\n",
    "test_neg_path = \"../data/inria_test_neg.txt\"\n",
    "test_pos_path = \"../data/inria_test_pos.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic classifier above can (should) achieve a success rate of 89% for the positive and less than 2% error for the negative images respectively. \n",
    "\n",
    "Can you tweak the HOG parameters to improve a little bit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# ....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, from the description above, what we created is no more than a *classifier* for only 2 classes (binary classifier): human(human-shaped) or not. In order to create a functional Person detector for arbitrary images or video sequences, some engineering techniques (heuristics) need to be implemented. \n",
    "\n",
    "Pretty much as in template matching, in order to find a person in an arbitrary image you will need to: \n",
    "\n",
    "* Slide your classifier over the full area of the image.\n",
    "* Detect possible matchings. \n",
    "* Report them as positive or negatives\n",
    "* And optionally, repeat the procedure above in different scales, to assure multiscale detection!.\n",
    "\n",
    "The procedure is nicely depicted in the image below for face detection.\n",
    "\n",
    "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2015/03/sliding-window-animated-adrian.gif\"> \n",
    "</img>\n",
    "Image taken from: https://www.pyimagesearch.com\n",
    "\n",
    "For now, we will leave those implementation details for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Face Recognition\n",
    "(*30 points*)\n",
    "\n",
    "In this section we will see how it is possible to recognize objects. The study case will target face regognition. Two approaches will be explored, `Eigen Faces` and `Fisher Faces` respectively. These methods are both based dimensional reduction technics listed below:\n",
    "\n",
    "- Principal Component Analysis ([*PCA*](http://www.utdallas.edu/~herve/abdi-awPCA2010.pdf))\n",
    "- Linear Discriminant Analysis (*LDA*)\n",
    "\n",
    "In general, in order to train a recognizer, several steps are needed and can be grouped as follow:\n",
    "\n",
    "- Data preparation\n",
    "- Recognizer training\n",
    "- System validation\n",
    "\n",
    "It is important for the validation step to ensure that the system tested with **unseen** data. By unseen data we mean data that have not been used during the training phase, this will ensure a fair performance assessment without biais. However this does not garantie that the system will *generalize* well to other dataset.\n",
    "\n",
    "### 2.3. 1 Data Preparation\n",
    "\n",
    "Data preparation covers various aspect of pre-processing step for training a system. At first, the images need to be splitted into two disctinct subsets thant will be used for `training` and `testing`. In our experiment the dataset used is the *Yale dataset version B*, it includes a total of 38 different identity (*i.e. subject*) each having 20 images undergoing different illumination condition for a total of 760 samples. The splitt will be done by chosing randomly samples from each subjects and placed into the corresponding subset, special care need to be taken in order to avoid having the same example multiple time.\n",
    "\n",
    "The first task is to gather the labels and the images that will be used to train the system. One solution is to store these information into a dictionary where the identity is the key and the pathes to the images for this subject are the values. \n",
    "\n",
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Scan for images in a given `path` and extract the label as well\n",
    "\n",
    "    :param path:    Path where YaleB dataset is stored\n",
    "    :return:        Dictionary storing the ID and a list of images for this ID\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    # Scan folder\n",
    "    dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    # Query images for each subject and extract the subject's ID\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # ....\n",
    "    \n",
    "        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_dataset(os.path.join('..', 'data', 'yaleB'))\n",
    "assert(len(data) == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all the images and labels have been gathered, the next step is to split into two subsets, the train set and test set. The training set will be composed of $75\\%$ of the images of each subject and the remaining $25\\%$ will be used as test set.\n",
    "\n",
    "Once again the two subsets information will be stored into two separates disctionary similar to what has been done earlier.\n",
    "\n",
    "**Your answer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, ratio):\n",
    "    \"\"\"\n",
    "    Splipt randomly a dataset into two subset. The ratio provide the distribution for each subset\n",
    "\n",
    "    :param data:    Overall dataset\n",
    "    :param ratio:   Split ratio\n",
    "    :return:        Two dictionaries, train/test\n",
    "    \"\"\"\n",
    "    train = {}\n",
    "    test = {}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # ....\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    # return subsets    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_dataset(data, 0.75)\n",
    "assert(len(train) == 38)\n",
    "assert(len(test) == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with these two subsets we can load the images and extract features from them. In this case the pixel intensity will be used as feature, therefore for an image $I \\in \\mathbb{R}^{ w \\times h}$ the feature vector will have a size of $wh$. This value can be quite large very easily, therefore all images will be downsampled by a factor of $2$.\n",
    "\n",
    "All the training samples will be concatenated into a single matrix where each row is an image (*i.e. flattened*) with dimensions $N \\times K$ where $N$ is the number of samples and $K$ is the dimension of a single image, $K = \\frac{wh}{4}$. The corresponding labels will also be concatenated into a single vector of dimension $N \\times 1$.\n",
    "\n",
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(subset):\n",
    "    \"\"\"\n",
    "    Load images into one single matrix where each row is one single image (flattened). The final dimensions is [N x K]\n",
    "    where N is the numper of samples available and K is the number of pixel in one image. The original image is first\n",
    "    downspample by a factor of 2\n",
    "\n",
    "    Labels are also exported into a single vector of dimensions [N x 1]\n",
    "\n",
    "    :param subset: Dictionary storing labels/images\n",
    "    :return:       Data matrix and label vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # data = ...\n",
    "    # label = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # ....\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "    # Done\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training images into memery\n",
    "train_img, train_label = load_images(train)\n",
    "# Sanity check\n",
    "assert(train_img.shape[0] == train_label.size)\n",
    "# Output number of samples\n",
    "print(\"There is a total of {} samples for the training set\".format(train_label.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3.2 Eigenfaces\n",
    "\n",
    "To perform recognition, all pixel's intensities are used as feature vector. The dimension of theses descriptors will be large, therefore a *clever* representation of the data, called subspace, is needed. \n",
    "\n",
    "This subspace is computed using *Principal Component Analysis* method in order to extract meaningfull information and reduce the dimension of the problem. The *PCA* approach is completely unsupervised and extract the directions, or *basis*, where the variation in the data are the largest inside the feature space. \n",
    "\n",
    "Since we are interested in the variation in the data, the first step is to remove the commmon information present in all samples by subtracting the **average face**. The average is computed using all training samples $I_i$ as follow:\n",
    "\n",
    "$$\n",
    "\\bar{\\boldsymbol{I}} = \\frac{1}{N_t} \\sum_{i=0}^{N_t} \\boldsymbol{I}_i\n",
    "$$\n",
    "\n",
    "where $N_t$ is the total number of training samples and $I_i$ is a specific training sample. Then each samples $I_i$ are normalized as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\phi_i} = \\boldsymbol{I}_i - \\bar{\\boldsymbol{I}}\n",
    "$$\n",
    "\n",
    "With all samples normalized, we need to find a set of orthognonal basis which best explain the distribution of our data. To do so we compute the eigendecomposition of the covariance matrix of the normalized samples.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{C} = \\frac{1}{N_t - 1} \\sum_{i}^{N_t} \\boldsymbol{\\phi}_i\\boldsymbol{\\phi}_i^{\\top} = \\frac{1}{N_t - 1} \\boldsymbol{\\Phi\\Phi}^{\\top}, \\quad \\text{where } \\Phi = \\left[\\boldsymbol{\\phi}_0, \\dots, \\boldsymbol{\\phi}_{N} \\right]\n",
    "$$\n",
    "\n",
    "Find the eigenvectors $u_k$ and the eigenvalues $\\lambda_k$. \n",
    "\n",
    "So far the dimensions of the problem have not been reduced. Moreover the size of the covariance matrix will be $K \\times K$ with $K = \\frac{wh}{4}$. Therefore we will find $K$ eigenvectors representing the variation in our data. To reduce the dimension we will select only the eigenvectors that contribute the most to the variation and dropping the one with little influence. Doing so will reduce the dimension of the subspace to $K \\times K_m$.\n",
    "\n",
    "The question is how to properly determine this $K_m$ value. It can be done by using the eigenvalues computed earlier. These values are representing the energy each vector contribute for. Therefore it is possible to dertmine the number of basis to select in order to retain a certain amount of energy.\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{k=0}^{K_m}\\lambda_k}{\\sum_{i=0}^{K}\\lambda_i} < \\Theta\n",
    "$$\n",
    "\n",
    "Where $\\Theta$ represents the amount of energy to retain, which usually is around $95\\%$ but can vary depending on the application. \n",
    "Finally the subspace is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{U} = \\left[\\boldsymbol{u}_0, \\dots, \\boldsymbol{u}_{K_m}\\right], \\quad \\boldsymbol{U} \\in \\mathbb{R}^{K \\times K_m}\n",
    "$$\n",
    "\n",
    "In practice, the PCA decomposition is computed using `sklearn.decomposition.PCA`, more information can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "\n",
    "For now, compute the face subspace on the training data by retaining $95\\%$ of the variance present in the training data.\n",
    "Once the subspace is computed, display the first $8$ modes or *eigenfaces* and comment on what you see, what do you thing are the limitations of such approach ?\n",
    "\n",
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our freshly subspace computed we can try to visualize if it separates properly the different subjects. Having proper plot of an $N$ dimensional space is not feasable, however we can use only a few components (*i.e. 2 or 3*) of our projected samples to visualize them. \n",
    "\n",
    "\n",
    "What we want is a subspace that is able to separates and clusters properly each subjects in order to avoid miss recognition. The code snippet below shows the $3^{rd}$ and $4^{th}$ components on a $2D$ plane.\n",
    "\n",
    "What do you think about it, do we have clean inter-subject separation ?\n",
    "\n",
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ....\n",
    "# Poject data onto subspace\n",
    "proj_train = ...\n",
    "\n",
    "\n",
    "# Visualize \n",
    "# Colors for distinct individuals\n",
    "colors = LabelEncoder().fit_transform(train_label.ravel())\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(proj_train[:, 2], proj_train[:, 3], c=colors)\n",
    "plt.xlabel('PC2')\n",
    "plt.ylabel('PC3')\n",
    "plt.title('Trainset clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our learned subspace we need to defined the corresponding representation for the training samples. This can be done by projecting them into the eigen subspace (*i.e. eigenface*) as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\omega}_i = \\boldsymbol{U}^\\top \\boldsymbol{\\phi}_i\n",
    "$$\n",
    "\n",
    "In the training set, there are multiple samples avaible for each subject. Their projection won't be exactly the same, therefore we need to have a more generic representation of each person. To do so, we average all representation of the specific person to have his general representation.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Omega}_k = \\frac{1}{N_k} \\sum_{p} \\boldsymbol{\\omega}_p\n",
    "$$\n",
    "\n",
    "where $N_k$ is the number of samples for subject $k$, $\\boldsymbol{\\omega}_p$ represents the projected samples of subject $k$ respectively. In our case, $k$ goes from $0$ to $37$.\n",
    "\n",
    "Now implement the function below that computes each subject's centroid, and returned their corresponding labels as well.\n",
    "\n",
    "**Your answer**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(data, trsfrm, label):\n",
    "    \"\"\"\n",
    "    Given a list of training samples, compute the centroids of each uniques labels\n",
    "\n",
    "    :param data:    Matrix with all feature vectors stored as row\n",
    "    :param trsfrm:  Embeddings to use (object with `transform(X)` method available such as PCA/LDA from sklearn)\n",
    "    :param label:   List of corresponding labels\n",
    "    :return:        Centroids, unique labels\n",
    "    \"\"\"\n",
    "\n",
    "    # extract unique label\n",
    "    unique_lbl = np.unique(label)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # ....\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    # Done\n",
    "    return centroids, unique_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define centroids\n",
    "train_centroids, train_centroid_label = compute_centroids(train_img, pca, train_label)\n",
    "assert(train_centroids.shape[0] == 38)\n",
    "assert(train_centroid_label.shape[0] == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've learned a face representation and computed the descriptor for the training samples. Now we can us them to recognize face. To do so the first step is to bring the *new* sample into our face *subspace* by projection, similarly  to what has been done before:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\omega}_n = \\boldsymbol{U}^{\\top}(\\boldsymbol{I}_n - \\bar{\\boldsymbol{I}})\n",
    "$$\n",
    "\n",
    "where $U$ is the face subspace, $\\bar{\\boldsymbol{I}}$ is the average face learned on the training data and $\\boldsymbol{I}_n$ is the new sample to recognize.\n",
    "\n",
    "Once the sample is in the same subspace as the training samples, we can measure its **similarity** (*distance*) with the centroids $\\boldsymbol{\\Omega}_k$ computed before. The predicted label will be the one corresponding the the closest centroid. \n",
    "\n",
    "$$ \n",
    "min \\left|\\left| \\boldsymbol{\\omega}_n - \\boldsymbol{\\Omega}_k \\right|\\right| \\quad \\forall k \\in \\{Train\\}\n",
    "$$\n",
    "\n",
    "Using the prototype below implement a function that predict each labels of new samples.\n",
    "\n",
    "**Your answer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(trsfrm, centroids, centroids_label, samples):\n",
    "    \"\"\"\n",
    "    Perform object recognition on a given list of ``amples\n",
    "\n",
    "    :param trsfrm:          Embeddings to use (object with `transform(X)` method available such as PCA/LDA from sklearn)\n",
    "    :param centroids:       List of centroids learned in training phase\n",
    "    :param centroids_label: Label corresponding to the centroids\n",
    "    :param samples:         List of samples to recognize\n",
    "    :return:                Predicted labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Define output container\n",
    "    pred = np.zeros((samples.shape[0], 1), dtype=np.float32)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # ....\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    # Done\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to valid the implementation is to try the whole system on the training set. The expected recognition accuracy should by close to 100% depending on the task difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognize training set\n",
    "pca_train_pred = recognize(pca, train_centroids, train_centroid_label, train_img)\n",
    "\n",
    "# Compute performance\n",
    "n_err = np.count_nonzero(np.where(pca_train_pred != train_label))\n",
    "train_acc = 1.0 - n_err / train_label.size\n",
    "print(\"The recognition accuracy on the trainig set is {:.2f}\".format(train_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the recognition accuracy on the testing set and comments on what you see\n",
    "\n",
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring accuracy of the system is a good indicator of the overall performance but does not indicate where the system is performing poorly. This can be quantified using **Confusion Matrix**. It describes the performance of classification model and shows how the system is confused for each samples in the training set.\n",
    "\n",
    "Such representation can be computed using `sklearn.metrics.confusion_matrix`, details are provided [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
    "\n",
    "**Your anser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Fisherface\n",
    "\n",
    "The subspace computed before with *PCA* was looking at directions where the variation in the data is maximum without paying attention to the class each data point belongs to. Therefore this approach is unsupervised. The major drawback is that the class separability is not garantee to be optimum. \n",
    "\n",
    "The approach of *Linear Discriminant Analysis* is to find a subspace where the variation is large (*i.e. similar to PCA*) but also to maximize the inter-class separability by taking into account each sample's label. The figure below shows an example:\n",
    "\n",
    "<img src=\"../data/lda_example.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Such subspace can be computed as follow:\n",
    "\n",
    "- Compute the scatter matrices (*intra-classes* / *inter-classes*)\n",
    "- Compute the eigenvectors / eigenvalues\n",
    "- Select the $K_m$ largest eigenvalues and their corresponding eigenvectors\n",
    "\n",
    "Given a set of samples $\\boldsymbol{I}_0, \\dots, \\boldsymbol{I}_N$ and their corresponding labels $y_0, \\dots, y_N$, the intra-class scatter matrix is computed as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_w = \\sum_{i=1}^N \\left(\\boldsymbol{I}_i - \\boldsymbol{\\mu}_{y_i}\\right) \\left(\\boldsymbol{I}_i - \\boldsymbol{\\mu}_{y_i}\\right)^{\\top}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mu}_{k}$ is the sample mean of the $k^{th}$ class.\n",
    "Then the inter-class scatter matrix is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_b = \\sum_{k=1}^{m} n_k (\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})^{\\top}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of classes, $\\boldsymbol{\\mu}$ is the overall sample average and $n_k$ is the number of samples in the $k^{th}$ class.\n",
    "\n",
    "Finally the subspace $\\boldsymbol{W}$ can be computed by solving the following generalizeed eigenvalue problem:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_b \\boldsymbol{W} = \\lambda \\boldsymbol{S}_w \\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "Finally at most $m-1$ generalized eigenvectors are useful to discriminate between $m$ classes.\n",
    "\n",
    "In practice, such decomposition can be computed using `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, more information available [here](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html).\n",
    "\n",
    "Now compute the *LDA* subspace similar to what you have done before and display the first 8 basis (*i.e. Fisherface*).\n",
    "\n",
    "**Your answer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we have done before visualize the subspace created by the *LDA* decomposition. \n",
    "\n",
    "What do you see, is it better than before ?\n",
    "\n",
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ....\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what has been done before, compute the recognition accuracy on the *training*/*testing* set for the *LDA* recognizer.\n",
    "\n",
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the *Confusion Matrix* and comment on the result you have\n",
    "\n",
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have implemented / tested two approaches for face recognition which one works the best and why ? What's are the pro/cons of each method ?\n",
    "\n",
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
